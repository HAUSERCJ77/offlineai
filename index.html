<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8"/>
  <title>Offline AI Chat</title>
  <link rel="manifest" href="manifest.json">
  <style>
    body { background:#0f1724; color:#e6eef8; font-family:sans-serif; margin:0; display:flex; flex-direction:column; height:100vh; }
    header { padding:10px; background:#1a2333; font-weight:bold; }
    #chat { flex:1; overflow-y:auto; padding:10px; display:flex; flex-direction:column; gap:8px; }
    .msg { padding:8px 12px; border-radius:10px; max-width:70%; }
    .user { background:#2f3c54; align-self:flex-end; }
    .bot { background:#1f2b42; align-self:flex-start; }
    footer { display:flex; gap:6px; padding:10px; border-top:1px solid #333; }
    input { flex:1; padding:8px; border-radius:6px; border:none; }
    button { padding:8px 12px; border:none; border-radius:6px; background:#7c5cff; color:white; cursor:pointer; }
  </style>
</head>
<body>
  <header>⚡ Offline AI Chat (WebGPU)</header>
  <main id="chat"><div class="msg bot">Loading model…</div></main>
  <footer>
    <input id="prompt" placeholder="Type your question..."/>
    <button id="send">Send</button>
  </footer>

<script type="module">
if ("serviceWorker" in navigator) {
  navigator.serviceWorker.register("./service-worker.js")
    .then(() => console.log("Service worker registered."));
}

// Import WebLLM runtime
import * as webllm from "https://cdn.jsdelivr.net/npm/@mlc-ai/web-llm@latest/dist/web-llm.mjs";

const chat = document.getElementById("chat");
const promptBox = document.getElementById("prompt");
const sendBtn = document.getElementById("send");

let modelHandle;

// Tiny model hosted on Hugging Face (~30MB)
const modelPath = "https://huggingface.co/Xenova/tiny-llama-fast-tokenizer/resolve/main/";

async function initModel() {
  try {
    // Choose backend: 'webgpu' if available, otherwise 'wasm'
    const backend = navigator.gpu ? "webgpu" : "wasm";
    if (backend === "wasm") appendBot("⚠️ WebGPU not available, falling back to CPU (slower)");

    modelHandle = await webllm.init({
      model: modelPath,
      backend: backend,
      temperature: 0.7,
      max_output_tokens: 128
    });

    appendBot("✅ Model loaded. You can now go offline and reload this page!");
  } catch (e) {
    console.error(e);
    appendBot("❌ Failed to load model.");
  }
}


function appendUser(text) {
  let d=document.createElement("div"); d.className="msg user"; d.textContent=text; chat.appendChild(d); chat.scrollTop=chat.scrollHeight;
}
function appendBot(text) {
  let d=document.createElement("div"); d.className="msg bot"; d.textContent=text; chat.appendChild(d); chat.scrollTop=chat.scrollHeight;
}

async function sendPrompt() {
  const text = promptBox.value.trim();
  if (!text) return;
  appendUser(text);
  appendBot("…thinking");
  promptBox.value = "";

  try {
    const res = await webllm.chat(modelHandle, text);
    chat.lastChild.remove();
    appendBot(res.text || String(res));
  } catch (err) {
    chat.lastChild.remove();
    appendBot("⚠️ Error: " + err.message);
  }
}

sendBtn.onclick = sendPrompt;
promptBox.addEventListener("keydown", (e)=>{ if (e.key==="Enter") sendPrompt(); });

initModel();
</script>
</body>
</html>
